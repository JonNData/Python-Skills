{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP GUIDE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonNData/Python-Skills/blob/master/NLP_GUIDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24g276mIj3c",
        "colab_type": "text"
      },
      "source": [
        "#Basic NLP Reference Guide \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xRRqVrCIrf-",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHZCXRhbIuW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import genism\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from genism.utils import simple_preprocess\n",
        "from genism.parsing.preprocessing import STOPWORDS\n",
        "from genism import corpora\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "import pyLDAvis.gensim\n",
        "from gensim.models.coherencemodel import CoherenceModel\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMHv-9odLoBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#regex pattern which keeps only alphanumeric characters:\n",
        "re.sub('[^a-zA-Z 0-9]', '', INSERTNAMEHERE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-uR7fWNMkhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function which takes a corpus of a document and returns a dataframe of word counts to analyze:\n",
        "def count(docs):\n",
        "\n",
        "        word_counts = Counter()\n",
        "        appears_in = Counter()\n",
        "        \n",
        "        total_docs = len(docs)\n",
        "\n",
        "        for doc in docs:\n",
        "            word_counts.update(doc)\n",
        "            appears_in.update(set(doc))\n",
        "\n",
        "        temp = zip(word_counts.keys(), word_counts.values())\n",
        "        \n",
        "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "        total = wc['count'].sum()\n",
        "\n",
        "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
        "        \n",
        "        wc = wc.sort_values(by='rank')\n",
        "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
        "\n",
        "        t2 = zip(appears_in.keys(), appears_in.values())\n",
        "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
        "        wc = ac.merge(wc, on='word')\n",
        "\n",
        "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
        "        \n",
        "        return wc.sort_values(by='rank')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB6o7ldOMsv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGymWabKqLZ",
        "colab_type": "text"
      },
      "source": [
        "##Tokenization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLSMhT-wKtME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union(['insert your stop words here'])\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "\n",
        "def tokenize(dataframe):\n",
        "    tokens = []\n",
        "    \"\"\" Update those tokens w/o stopwords\"\"\"\n",
        "    for doc in tokenizer.pipe(df['reviews.text'], batch_size=500):\n",
        "      \n",
        "        doc_tokens = []\n",
        "\n",
        "        for token in doc:\n",
        "            if token.text.lower() not in STOP_WORDS:\n",
        "                doc_tokens.append(token.text.lower())\n",
        "\n",
        "        tokens.append(doc_tokens)\n",
        "        \n",
        "    return tokens\n",
        "\n",
        "?df?['tokens'] = tokenize(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vj-BHmTRUB",
        "colab_type": "text"
      },
      "source": [
        "##Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5ng84GOTSxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"wolf\", \"wolves\"]\n",
        "\n",
        "for word in words:\n",
        "    print(ps.stem(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogH9dgSEK9OV",
        "colab_type": "text"
      },
      "source": [
        "##Lemmatization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3SKedGXLCLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lemmas(text):\n",
        "\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Something goes here :P\n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_!= 'PRON'):\n",
        "            lemmas.append(token.lemma_)\n",
        "    \n",
        "    return lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sicEfHHXOL15",
        "colab_type": "text"
      },
      "source": [
        "##Vector Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUT8URzMOScG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count Vectorizer\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "#learn vocab\n",
        "vect.fit(data)\n",
        "\n",
        "#get sparse dtm\n",
        "dtm = vect.transform(data0)\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_uKLRkxOUJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF-IDF Vectorizer\n",
        "\n",
        "#instantiate vectorizer object\n",
        "tfidf = TfidfVectorizer(stop_words='english', \n",
        "                        ngram_range=(1,2),\n",
        "                        max_df=.97,\n",
        "                        min_df=3,\n",
        "                        tokenizer=tokenize)\n",
        "\n",
        "# Create a vocabulary and get word counts per document\n",
        "dtm = tfidf.fit_transform(data)\n",
        "\n",
        "# Get feature names to use as dataframe column headers\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "# View Feature Matrix as DataFrame\n",
        "dtm.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ0bdMlsUehn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA\n",
        "\n",
        "def get_word_vectors(words):\n",
        "  #converts a list of words into their word vectors\n",
        "  return [nlp(word).vector for word in words]\n",
        "\n",
        "words = ['car', 'truck', 'suv', 'race', 'elves', 'dragon', 'sword', 'king', 'queen', 'prince', 'horse', 'fish' , 'lion', 'tiger', 'lynx', 'potato']\n",
        " \n",
        "# intialise pca model and tell it to project data down onto 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# fit the pca model to our 300D data, this will work out which is the best \n",
        "# way to project the data down that will best maintain the relative distances \n",
        "# between data points. It will store these intructioons on how to transform the data.\n",
        "pca.fit(get_word_vectors(words))\n",
        "\n",
        "# Tell our (fitted) pca model to transform our 300D data down onto 2D using the \n",
        "# instructions it learnt during the fit phase.\n",
        "word_vecs_2d = pca.transform(get_word_vectors(words))\n",
        "\n",
        "# let's look at our new 2D word vectors\n",
        "word_vecs_2d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4-uuAZOXcq",
        "colab_type": "text"
      },
      "source": [
        "##Document Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkgio5XHOZoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic NLP pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "pipe = Pipeline([('vect', vect), ('clf', clf)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7wxFMf9Ob-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Singular value decomposition\n",
        "svd = TruncatedSVD(n_components=100, # Just here for demo. \n",
        "                   algorithm='randomized',\n",
        "                   n_iter=10)\n",
        "\n",
        "\n",
        "# LSI: Latent semantic indexing\n",
        "lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
        "\n",
        "\n",
        "# Pipe\n",
        "pipe = Pipeline([('lsi', lsi), ('clf', rfc)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COmL54suU9MC",
        "colab_type": "text"
      },
      "source": [
        "##Text Feature Extraction & Classification Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu50Q3A5VITg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create pipeliune components\n",
        "vect = TfidVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "rfc = RandomForestClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_csG2DMBVbS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the pipeline:\n",
        "pipe = Pipeline([ \n",
        "                 #vectorizer\n",
        "                 ('vect', vect), \n",
        "                 #classifier\n",
        "                 ('clf', rfc)\n",
        "                 ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtngbPfPVui6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {\n",
        "    'vect__max_df': ( 0.75, 1.0),\n",
        "    'vect__min_df': (.02, .05),\n",
        "    'vect__max_features': (500,1000),\n",
        "    'clf__n_estimators':(5, 10,),\n",
        "    'clf__max_depth':(15,20)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(data.data, data.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFSPKyH8VwlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c2Xxc1JVxrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search.predict([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Rqz8rkWGqV",
        "colab_type": "text"
      },
      "source": [
        "##Latent Semantic Indexing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Ve4k-_WIEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "svd = TruncatedSVD(n_components=100, # Just here for demo. \n",
        "                   algorithm='randomized',\n",
        "                   n_iter=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg_EW6hOWM_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = { \n",
        "    'lsi__svd__n_components': [10,100,250],\n",
        "    'lsi__vect__max_df':[.9, .95, 1.0],\n",
        "    'clf__n_estimators':[5,10,20]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7lvkFfAWO63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSI\n",
        "lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
        "\n",
        "\n",
        "# Pipe\n",
        "pipe = Pipeline([('lsi', lsi), ('clf', rfc)])\n",
        "\n",
        "print(pipe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecY_Jm1HWPke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit\n",
        "grid_search = GridSearchCV(pipe,params, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(data.data, data.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwvyD5FEWWkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llQhyonDOdJL",
        "colab_type": "text"
      },
      "source": [
        "##Word Embeddings with Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2awlkoxxOe5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"Two bananas in pyjamas\")\n",
        "\n",
        "def get_word_vectors(docs):\n",
        "    return [nlp(doc).vector for doc in docs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX35nKD8OnT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = get_word_vectors(train['description'])\n",
        "\n",
        "len(X) == len(data.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BJQFywWWyUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = get_word_vectors(test['description'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVdXLgLFWzxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfc.fit(X, train['ratingCategory'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAZ_Md-DW3Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfc.score(X, train['ratingCategory'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um79vICEW4xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfc.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-u4uyM6W52q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['ratingCategory'] = rfc.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfvGbeozW7aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test[['id', 'ratingCategory']].to_csv('testSolutionSubmission.csv', header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKIwrBVIOod8",
        "colab_type": "text"
      },
      "source": [
        "##Topic Modelings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83bTalcvOrEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Genism LDA \n",
        "\n",
        "id2word = corpora.Dictionary(tokens)# tokens represents a list of tokenized strings lists\n",
        "\n",
        "# Let's remove extreme values from the dataset\n",
        "id2word.filter_extremes(no_below=5, no_above=0.95)\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in tokens]\n",
        "\n",
        "lda = LdaMulticore(corpus=corpus,\n",
        "                   id2word=id2word,\n",
        "                   random_state=723812,\n",
        "                   num_topics = 15,\n",
        "                   passes=10,\n",
        "                   workers=8\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qq6PdLoOsyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "pyLDAvis.gensim.prepare(lda, corpus, id2word)\n",
        "\n",
        "distro = [lda[d] for d in corpus]\n",
        "\n",
        "def update(doc):\n",
        "        d_dist = {k:0 for k in range(0,15)}\n",
        "        for t in doc:\n",
        "            d_dist[t[0]] = t[1]\n",
        "        return d_dist\n",
        "    \n",
        "new_distro = [update(d) for d in distro]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}